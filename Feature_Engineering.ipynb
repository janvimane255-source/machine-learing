{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Whate is parameter?\n",
        "\n",
        "ans- In Python, a parameter is a variable defined in the function declaration or definition. It acts as a placeholder for the value (known as an argument) that will be passed to the function when it is called. Parameters allow functions to accept input data and perform operations based on that data.\n",
        "Example:\n",
        "Pythondef greet(name):  # 'name' is the parameter\n",
        "    print(f\"Hello, {name}!\")\n",
        "\n",
        "Here, name is the parameter. When you call the function and pass a value (argument), it replaces the parameter.\n",
        "Key Points:\n",
        "\n",
        "\n",
        "Parameters vs. Arguments:\n",
        "\n",
        "Parameters: Defined in the function header (e.g., name in the example above).\n",
        "Arguments: Actual values passed to the function when calling it (e.g., \"Alice\" in greet(\"Alice\")).\n",
        "\n",
        "\n",
        "\n",
        "Types of Parameters:\n",
        "\n",
        "Positional Parameters: Standard parameters that must be provided in the correct order.\n",
        "Default Parameters: Parameters with default values, used when no argument is provided.\n",
        "Keyword Parameters: Parameters explicitly named during the function call.\n",
        "Variable-Length Parameters: Allow passing an arbitrary number of arguments using *args (for positional arguments) or **kwargs (for keyword arguments).\n",
        "\n",
        "\n",
        "\n",
        "Example with Multiple Types:\n",
        "Pythondef introduce(name, age=25, *hobbies, **details):\n",
        "    print(f\"My name is {name}, I am {age} years old.\")\n",
        "    print(f\"My hobbies are: {', '.join(hobbies)}\")\n",
        "    print(f\"Additional details: {details}\")\n",
        "\n",
        "introduce(\"John\", 30, \"reading\", \"cycling\", city=\"New York\", profession=\"Engineer\")\n",
        "\n",
        "This flexibility makes Python functions powerful and adaptable!\n"
      ],
      "metadata": {
        "id": "328nSdZUJh_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def introduce(name, age=25, *hobbies, **details):\n",
        "    print(f\"My name is {name}, I am {age} years old.\")\n",
        "    print(f\"My hobbies are: {', '.join(hobbies)}\")\n",
        "    print(f\"Additional details: {details}\")\n",
        "\n",
        "introduce(\"John\", 30, \"reading\", \"cycling\", city=\"New York\", profession=\"Engineer\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HJWJpj9U65N",
        "outputId": "29d54c76-b97e-4c35-a525-919c10410086"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name is John, I am 30 years old.\n",
            "My hobbies are: reading, cycling\n",
            "Additional details: {'city': 'New York', 'profession': 'Engineer'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2.What is correlation?\n",
        " What does negative correlation mean?\n",
        "\n",
        " ans- **Correlation is a statistical measure that describes the strength and direction of a relationship between two variables. Negative correlation means that as one variable increases, the other variable tends to decrease.**\n",
        "  \n",
        "  \n",
        "  \n",
        "## What is Correlation?  \n",
        "  \n",
        "**Correlation** is used to quantify and describe how two variables move in relation to each other. It provides information about both:  \n",
        "- **Direction of the relationship**: whether the variables tend to increase or decrease together.    \n",
        "- **Strength of the relationship**: how closely the variables are related.    \n",
        "  \n",
        "The most common measure of correlation is the **Pearson correlation coefficient** (denoted as *r*), which ranges from **-1 to +1**:  \n",
        "- *r = +1*: perfect positive correlation    \n",
        "- *r = -1*: perfect negative correlation    \n",
        "- *r = 0*: no correlation (variables are independent)    \n",
        "  \n",
        "Correlation is often visualized using a **scatter plot**, where each point represents an observation with values for the two variables.\n",
        "## What Does Negative Correlation Mean?  \n",
        "  \n",
        "A **negative correlation** indicates an **inverse relationship** between two variables. In other words, **as one variable increases, the other tends to decrease**, and vice versa. For example:  \n",
        "- The number of hours spent watching TV might be negatively correlated with exam scores; more TV hours often correspond to lower scores.    \n",
        "- The outside temperature and the use of heating might have a negative correlation; as temperature rises, heating use decreases.    \n",
        "  \n",
        "The closer the correlation coefficient is to **-1**, the stronger the negative relationship. A coefficient near **0** indicates a very weak or no inverse relationship.  \n",
        "## Important Notes  \n",
        "  \n",
        "- **Correlation does not imply causation**: just because two variables are correlated (positively or negatively) does not mean one causes the other.    \n",
        "- **Linear relationships** are typically measured by Pearson’s correlation; non-linear relationships may exist even if the correlation is small.    \n",
        "- Correlation is useful for identifying patterns, making predictions, and determining which variables are associated in data analysis.  \n",
        "  \n",
        "Understanding correlation, including negative correlation, is fundamental for **statistics, research, and data interpretation**.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "v00OJpiXJrpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Define Machine Learning. What are the main components in Machine Learning?\n",
        "\n",
        "ans- **Machine Learning (ML) is a branch of artificial intelligence where systems learn patterns from data to make predictions or decisions without being explicitly programmed.**\n",
        "  \n",
        "  \n",
        "  \n",
        "## Definition of Machine Learning  \n",
        "Machine Learning is a field of computer science and artificial intelligence that focuses on **developing algorithms and models that enable computers to learn from and make decisions based on data**. Instead of being programmed with explicit rules, ML systems **learn from examples** and improve their performance over time as they are exposed to more data.\n",
        "ML can be categorized broadly into three types:\n",
        "- **Supervised Learning:** The model learns from labeled data to predict outcomes.    \n",
        "- **Unsupervised Learning:** The model identifies patterns or structures in unlabeled data.    \n",
        "- **Reinforcement Learning:** An agent learns to make decisions by interacting with an environment and receiving feedback in the form of rewards or penalties.  \n",
        "  \n",
        "## Main Components of Machine Learning  \n",
        "A typical Machine Learning system consists of several key components:\n",
        "1. **Data**: High-quality, relevant data is the foundation of ML. Data can include structured data (like spreadsheets) or unstructured data (like text, images, audio). Data preprocessing is crucial to clean, normalize, and transform the data into a suitable format for the model.    \n",
        "  \n",
        "2. **Features**: Features are measurable attributes or properties of the data that are used as input to the ML model. **Feature engineering** involves selecting, transforming, or creating features to improve model performance.    \n",
        "  \n",
        "3. **Model/Algorithm**: The model is the mathematical or computational representation that learns patterns from data. Common algorithms include linear regression, decision trees, neural networks, and support vector machines. The choice of algorithm depends on the type of problem (classification, regression, clustering, etc.).    \n",
        "  \n",
        "4. **Training Process**: During training, the model uses data to adjust its internal parameters to minimize error or maximize performance according to a defined objective function (loss function). Techniques such as gradient descent are often used for optimization.    \n",
        "  \n",
        "5. **Evaluation Metrics**: After training, models are evaluated to assess how well they generalize to new, unseen data. Metrics vary by task, e.g., accuracy, precision, recall, F1-score for classification, or mean squared error for regression.    \n",
        "  \n",
        "6. **Prediction/Inference**: Once trained and validated, the model is used to make predictions or decisions on new data. This is the practical application phase where ML generates value.    \n",
        "  \n",
        "7. **Feedback Loop (Optional)**: Many ML systems are iteratively improved by collecting new data and feedback, which helps in fine-tuning the model to enhance performance and adapt to changing conditions.  \n",
        "  \n",
        "In summary, **Machine Learning enables computers to automatically learn from data**, and its effectiveness depends on high-quality data, appropriate features, a suitable model, rigorous training, and careful evaluation, supported by continuous improvement.\n",
        "\n"
      ],
      "metadata": {
        "id": "w43VRsYiJ6ji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  How does loss value help in determining whether the model is good or not?\n",
        "\n",
        "ans-\n",
        "  **The loss value quantifies how closely a machine learning model's predictions match the true outputs, and a lower loss generally indicates a better-performing model.**\n",
        "  \n",
        "  \n",
        "  \n",
        "### Understanding Loss Value  \n",
        "The **loss function** is a mathematical measure of the difference between the model's predicted outputs and the actual targets. Common loss functions include **Mean Squared Error (MSE)** for regression, **Cross-Entropy Loss** for classification, and **Mean Absolute Error (MAE)** for robust regression. The value computed by the loss function is the **loss value**.\n",
        "- **High loss value**: The predictions deviate significantly from the true targets, suggesting poor model fit.    \n",
        "- **Low loss value**: The predictions are closer to the targets, indicating the model is learning effectively.  \n",
        "  \n",
        "### Role in Assessing Model Quality  \n",
        "- **Training loss**: Shows how well the model fits the training data. A decreasing training loss over epochs indicates the model is learning from the data. However, a very low training loss alone does not guarantee good generalization.    \n",
        "- **Validation loss**: Calculated on unseen data to evaluate generalization. A low **validation loss** often correlates with better real-world performance. If training loss is low but validation loss is high, it may indicate **overfitting**.    \n",
        "  \n",
        "### Comparing Models  \n",
        "Loss values help in **model selection**. If two models are evaluated on the same loss function over the same dataset, the model with the **lower validation loss** is usually preferred. However, it's important to consider the scale of the loss function and whether it matches the application requirements.\n",
        "### Limitations  \n",
        "- Loss value alone may not fully capture performance, especially for classification tasks where metrics like **accuracy, F1-score, precision, and recall** may be more interpretable.    \n",
        "- Some models may achieve very low loss yet perform poorly in practical scenarios if the loss function does not align with business goals or decision thresholds.    \n",
        "  \n",
        "### Summary  \n",
        "**Loss value is a numerical measure of model error**: the smaller the loss, the better the predictions match the targets, both on training and validation sets. Monitoring both training and validation loss helps detect underfitting (high loss on both) and overfitting (low training loss but high validation loss), providing a clear indication of model quality and generalization ability.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2xCUySJjKBoQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are continuous and categorical variables?\n",
        "\n",
        "\n",
        "ans-**Continuous variables can take any numerical value within a range, while categorical variables represent discrete groups or categories.**\n",
        "  \n",
        "  \n",
        "  \n",
        "## Continuous Variables  \n",
        "**Continuous variables** are numerical variables that can take an infinite number of values within a given range. These values are measurable and usually obtained through measurement tools or instruments. Continuous variables are suitable for arithmetic operations such as addition, subtraction, and averaging. Examples include:  \n",
        "- Height of a person (e.g., 170.5 cm)    \n",
        "- Temperature in Celsius or Fahrenheit (e.g., 36.6°C)    \n",
        "- Time taken to complete a task (e.g., 12.3 minutes)    \n",
        "- Weight of an object (e.g., 75.8 kg)    \n",
        "  \n",
        "Continuous variables are often visualized using **histograms, line charts, or scatter plots**, which help in understanding their distribution and trends. They are fundamental in regression analysis, correlation studies, and other statistical modeling techniques.  \n",
        "## Categorical Variables  \n",
        "**Categorical variables**, also known as discrete or qualitative variables, represent distinct groups or categories rather than numerical values. These categories cannot be meaningfully measured with arithmetic operations like averaging, but they can be counted. Categorical variables can be further divided into:  \n",
        "- **Nominal variables**: Categories with no natural order (e.g., blood type: A, B, AB, O; gender: male, female, other)    \n",
        "- **Ordinal variables**: Categories with a meaningful order or ranking, but the differences between them are not necessarily uniform (e.g., education level: high school, bachelor's, master's, PhD; customer satisfaction: low, medium, high)    \n",
        "  \n",
        "Categorical variables are typically analyzed using **bar charts, pie charts, or frequency tables**, which display the number of observations in each category. They are commonly used in classification problems, demographic studies, and survey data analysis.  \n",
        "## Summary  \n",
        "In short, **continuous variables measure quantities with a potentially infinite range of values**, whereas **categorical variables classify observations into distinct groups or levels**. Understanding the distinction is essential for selecting appropriate statistical tests, visualization, and data modeling techniques.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VlhHdJoTKHBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.  How do we handle categorical variables in Machine Learning? What are the common t\n",
        " echniques?\n",
        "\n",
        " ans- **Categorical variables, which represent qualitative data, must be converted into numerical formats such as one-hot, label, ordinal, binary, frequency, or target encoding to be effectively used in machine learning models** [^1^] [^3^] [^7^].\n",
        "  \n",
        "  \n",
        "  \n",
        "## Understanding Categorical Data  \n",
        "Categorical data can be classified into two main types:  \n",
        "- **Nominal data**: Categories with no inherent order, e.g., color, gender, or product type.    \n",
        "- **Ordinal data**: Categories with a clear order or ranking, e.g., education level or satisfaction ratings [^1^] [^6^].  \n",
        "  \n",
        "Machine learning algorithms typically require numerical inputs, so encoding categorical data properly is critical to prevent misinterpretation and improve model accuracy [^3^].\n",
        "## Common Techniques for Encoding Categorical Data  \n",
        "  \n",
        "1. **Label Encoding**    \n",
        "  \n",
        "   Each category is assigned a unique integer value. Works well for ordinal data where the order matters. **Pros**: Simple, memory-efficient. **Cons**: Implies an order even for nominal data, which can bias models like linear regression or neural networks [^3^] [^4^].\n",
        "2. **One-Hot Encoding**    \n",
        "  \n",
        "   Creates binary columns for each category; only one column is “hot” (1) per observation. Best for nominal variables. **Pros**: Avoids false ordering, interpretable for most models. **Cons**: Can lead to high-dimensional sparse data when categories are many [^3^] [^4^] [^5^].\n",
        "3. **Ordinal Encoding**    \n",
        "  \n",
        "   Maps categories to integers while preserving the natural order, ideal for ordinal variables like ratings. **Pros**: Maintains order and reduces dimensionality. **Cons**: Not suitable for nominal data [^1^] [^6^].\n",
        "4. **Target Encoding (Mean Encoding)**    \n",
        "  \n",
        "   Replaces each category by the mean of the target variable for that category. Useful for high-cardinality features with predictive value. **Pros**: Captures the relationship to the target variable. **Cons**: Risk of overfitting; requires cross-validation and smoothing [^3^] [^4^].\n",
        "5. **Binary Encoding**    \n",
        "  \n",
        "   Converts category indices to binary codes and splits them across multiple columns. Efficient for high-cardinality data. **Pros**: Reduces dimensionality compared to one-hot encoding. **Cons**: Slightly more complex [^3^] [^5^].\n",
        "6. **Frequency or Count Encoding**    \n",
        "  \n",
        "   Replaces categories with their frequency or count within the dataset. **Pros**: Simple, memory-efficient, can capture importance of frequent categories. **Cons**: May lose category-specific semantics, can introduce data leakage if not handled properly [^4^] [^5^].\n",
        "7. **Hash Encoding**    \n",
        "  \n",
        "   Maps categories to a fixed-size vector using a hash function. Useful for very large datasets. **Pros**: Memory-efficient, prevents extremely large one-hot vectors. **Cons**: Possible hash collisions, less interpretable [^6^].\n",
        "## Best Practices  \n",
        "- **Select encoding based on data type**: Ordinal for ordered categories, one-hot for nominal categories.    \n",
        "- **High-cardinality features**: Use target encoding, binary encoding, or hashing to manage dimensionality [^7^].    \n",
        "- **Handle rare or unseen categories**: Group rare categories into “Other” or implement hierarchical categories [^7^].    \n",
        "- **Avoid data leakage**: When using target encoding, calculate statistics on training data only, not including validation or test sets [^3^].    \n",
        "- **Cross-validation**: Helps prevent overfitting especially for target encoding [^3^].    \n",
        "  \n",
        "Proper categorical variable handling ensures **models interpret inputs accurately, improve predictive power, and avoid bias**. Choosing the right encoding technique depends on the type of categorical data, feature cardinality, and the algorithm's sensitivity to numeric representation [^6^] [^8^].\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1ELX2YPmKLck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What do you mean by training and testing a dataset?\n",
        "\n",
        "ans- **Training a dataset means using part of your data to teach a machine learning model patterns, while testing a dataset evaluates how well the model performs on unseen data.**\n",
        "  \n",
        "  \n",
        "  \n",
        "## Training Dataset  \n",
        "The **training dataset** is a subset of the overall data that is used to **train or fit a model**. During training, the algorithm analyzes the data, identifies patterns, and adjusts its internal parameters to minimize errors. For example, in a classification task, the model will learn how the input features correspond to the target labels. A good training dataset should be **representative of the problem space** to allow the model to generalize effectively to new data.\n",
        "## Testing Dataset  \n",
        "The **testing dataset** is another subset of the data that the model **has not seen during training**. It is used to **evaluate the model’s performance** objectively. By applying the model to the test data, one can measure metrics such as accuracy, precision, recall, F1-score, or mean squared error, depending on the task. Testing ensures that the model is **not overfitting**, meaning it does not simply memorize the training data but generalizes to unseen instances.\n",
        "## Typical Workflow  \n",
        "1. **Split the data**: Usually, data is split into training (e.g., 70–80%) and testing (20–30%) sets. Sometimes a validation set is also used to tune hyperparameters.    \n",
        "2. **Train the model**: The model learns patterns from the training dataset.    \n",
        "3. **Validate (optional)**: Parameters may be adjusted using a validation set.    \n",
        "4. **Test the model**: Finally, the trained model is tested on unseen data to evaluate performance.  \n",
        "  \n",
        "This **train-test split** is fundamental in machine learning because it provides a clear way to assess whether the model will perform well in real-world or future scenarios, rather than just on the data it was trained on. Proper splitting and evaluation prevent overoptimistic performance estimates and guide improvements to the model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rgsOOz5xKR5Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is sklearn.preprocessing?\n",
        "\n",
        "ans-   \n",
        "  \n",
        "`sklearn.preprocessing` is a submodule in the **scikit-learn (sklearn)** library that provides utilities and functions to prepare your data before applying machine learning algorithms. Proper preprocessing often improves model performance, reduces training time, and ensures that different features are on comparable scales.\n",
        "## Key Functionalities of `sklearn.preprocessing`  \n",
        "  \n",
        "1. **Scaling Features**    \n",
        "  \n",
        "Some algorithms, like SVM, K-Nearest Neighbors, and gradient-based models, are sensitive to the scale of input features. `sklearn.preprocessing` provides tools such as:\n",
        "- `StandardScaler`: Standardizes features by removing the mean and scaling to unit variance (`z = (x - mean)/std`).  \n",
        "   - `MinMaxScaler`: Scales features to a fixed range, usually [0, 1].  \n",
        "   - `RobustScaler`: Scales features using statistics robust to outliers (median and IQR).  \n",
        "  \n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "X = [[1, 2], [2, 4], [3, 6]]\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "print(X_scaled)\n",
        "```\n",
        "2. **Encoding Categorical Variables**    \n",
        "  \n",
        "Machine learning models work with numerical data, so text/categorical labels need encoding:\n",
        "- `LabelEncoder`: Converts labels into numeric form (`['red','green'] -> [0,1]`).  \n",
        "   - `OneHotEncoder`: Converts categorical variables into binary vectors.  \n",
        "  \n",
        "```python\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "X = [['red'], ['green'], ['blue']]\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "X_encoded = encoder.fit_transform(X)\n",
        "print(X_encoded)\n",
        "```\n",
        "3. **Generating Polynomial Features**    \n",
        "  \n",
        "Transforms features into polynomial combinations for models that benefit from interaction terms:\n",
        "```python\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "X = [[1, 2], [3, 4]]\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "print(X_poly)\n",
        "```\n",
        "4. **Normalizing Data**    \n",
        "  \n",
        "Sometimes models benefit from normalized feature vectors:\n",
        "- `Normalizer`: Scales individual samples to unit norm (L1 or L2 norm).  \n",
        "  \n",
        "```python\n",
        "from sklearn.preprocessing import Normalizer\n",
        "X = [[4, 3], [1, 2]]\n",
        "normalizer = Normalizer()\n",
        "X_normalized = normalizer.transform(X)\n",
        "print(X_normalized)\n",
        "```\n",
        "## Summary    \n",
        "  \n",
        "`sklearn.preprocessing` is essential for:  \n",
        "- Scaling and normalizing numerical features.    \n",
        "- Encoding categorical data into numbers or vectors.    \n",
        "- Generating interaction and polynomial features.    \n",
        "- Ensuring uniformity in datasets for better model performance.    \n",
        "  \n",
        "By preprocessing data correctly, you often improve model training efficiency and predictive accuracy, making it a vital step in any machine learning pipeline.  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pJPcpilTKWCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.What is a Test set?\n",
        "\n",
        "ans-**A test set in Jira is a collection of related test cases grouped together to facilitate organization, execution, and management of testing activities within a project.** [^1^] [^2^]\n",
        "  \n",
        "  \n",
        "  \n",
        "## What is a Test Set?  \n",
        "  \n",
        "A **test set** serves as an organizational structure for test cases. It allows teams to group tests for a specific purpose or functionality, such as login, payment processing, or reporting features. Unlike hierarchical test repositories, a test set is typically a **flat list** of test cases, meaning each test is listed individually but can appear in multiple test sets [^1^] [^5^].\n",
        "## Purpose and Benefits  \n",
        "  \n",
        "- **Better organization**: Test sets categorize test cases, making it easier to locate, execute, and manage tests, especially in projects with a large number of cases [^8^].    \n",
        "- **Efficient execution tracking**: Associating test cases with a test set allows testers to execute and monitor results collectively rather than individually [^5^] [^9^].    \n",
        "- **Flexibility**: A single test case can belong to multiple test sets, enabling reuse across different testing scenarios or releases [^1^].    \n",
        "- **Enhanced traceability**: Grouping tests helps in linking them to related requirements, user stories, or defects for comprehensive coverage and reporting [^9^].    \n",
        "  \n",
        "## How Test Sets Work in Jira with Test Management Tools  \n",
        "  \n",
        "1. **Creation**: In Jira, using plugins like Xray or Zephyr, you can create a test set by selecting \"Test Set\" as the issue type and providing a summary and optional description [^6^].    \n",
        "2. **Adding Test Cases**: Test cases are attached to the test set, which can include new tests, existing tests, or even tests from other test sets [^5^] [^6^].    \n",
        "3. **Execution**: After creating a test set, it can be included in a **Test Execution** issue to run all contained test cases and record results efficiently [^5^] [^8^].    \n",
        "4. **Reporting**: Test sets allow dashboards and reports to show the execution status of multiple tests at once, helping managers and QA teams monitor progress quickly [^5^].    \n",
        "  \n",
        "## Example Use  \n",
        "  \n",
        "For instance, if an application has multiple functionalities, you might create separate test sets for:\n",
        "- \"Login Functionality Tests\"  \n",
        "- \"Shopping Cart Tests\"  \n",
        "- \"Payment Gateway Tests\"  \n",
        "  \n",
        "Each set contains the relevant test cases, helping testers perform grouped executions and simplifying tracking of which features have been verified [^9^].\n",
        "In summary, **test sets in Jira provide a practical way to manage, organize, and execute test cases**, making testing workflows more systematic, reusable, and traceable across different project stages [^1^] [^5^] [^9^].\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "K3WFVws8KaGR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. How do we split data for model fitting (training and testing) in Python?\n",
        " How do you approach a Machine Learning problem?\n",
        "\n",
        " ans- **In Python, you typically split your dataset into training and testing sets using `scikit-learn`’s `train_test_split` function, and a structured approach to machine learning involves data preparation, feature engineering, model selection, evaluation, and iteration.**\n",
        "  \n",
        "  \n",
        "  \n",
        "## Splitting Data in Python  \n",
        "  \n",
        "The standard way to split data for training and testing is using **`train_test_split`** from the `sklearn.model_selection` module:\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "# X: features, y: target variable  \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "Key points:\n",
        "- **`test_size=0.2`** indicates 20% of the data is reserved for testing; you can adjust it based on dataset size.    \n",
        "- **`random_state`** ensures reproducibility by fixing the random seed.    \n",
        "- Optionally, for classification problems, you can use `stratify=y` to maintain the same class distribution in both sets.    \n",
        "  \n",
        "For large datasets, you may also consider a **validation set** separate from the test set, often splitting the training set further:\n",
        "```python\n",
        "X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n",
        "# 0.25 of the training set becomes validation (~20% of total data)  \n",
        "```\n",
        "## General Approach to a Machine Learning Problem  \n",
        "  \n",
        "1. **Problem Definition**    \n",
        "  \n",
        "   Clearly define the business or research problem and determine the type of prediction (classification, regression, clustering, etc.).\n",
        "2. **Data Collection & Exploration**    \n",
        "  \n",
        "   Gather your data and perform exploratory data analysis (EDA). Key tasks include handling missing values, detecting outliers, and understanding feature distributions.\n",
        "3. **Data Preprocessing**    \n",
        "   - Normalize or standardize numerical features.    \n",
        "   - Encode categorical variables (one-hot encoding, label encoding).    \n",
        "   - Handle missing values and data imbalances.    \n",
        "  \n",
        "4. **Feature Selection & Engineering**    \n",
        "   - Identify informative features.    \n",
        "   - Create new features if necessary to improve model performance.  \n",
        "  \n",
        "5. **Model Selection & Training**    \n",
        "   - Choose suitable algorithms (e.g., Linear Regression, Random Forest, XGBoost, Neural Networks).    \n",
        "   - Train the model using the training set and tune hyperparameters with cross-validation.  \n",
        "  \n",
        "6. **Evaluation**    \n",
        "   - Evaluate model on the test set using metrics appropriate to the problem (accuracy, precision, recall, F1-score, RMSE, etc.).    \n",
        "   - Use learning curves to detect bias or variance issues.  \n",
        "  \n",
        "7. **Iteration & Optimization**    \n",
        "   - Refine features, try different algorithms, and optimize hyperparameters based on model performance.    \n",
        "   - Consider ensemble methods or advanced techniques if needed.  \n",
        "  \n",
        "8. **Deployment & Monitoring**    \n",
        "  \n",
        "   Once satisfied, deploy the model and implement monitoring to track real-world performance.\n",
        "Following this structured pipeline allows you to systematically build, evaluate, and improve ML models while avoiding common pitfalls such as overfitting or data leakage.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J5D6SAMkKenL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Why do we have to perform EDA before fitting a model to the data?\n",
        "\n",
        "ans-**Performing Exploratory Data Analysis (EDA) before fitting a model is essential to understand your data, detect issues, and make informed decisions that directly improve model performance.**\n",
        "  \n",
        "  \n",
        "  \n",
        "## Understanding the Data  \n",
        "  \n",
        "EDA is the process of **examining and visualizing your dataset** before applying any statistical or machine learning models. It allows you to determine the **distribution of variables, relationships among features, and patterns in the data**, which can influence the choice of model and preprocessing steps. Without this understanding, you risk building a model on flawed or suboptimal data.\n",
        "## Detecting Data Issues  \n",
        "  \n",
        "Through EDA, analysts can identify **missing values, outliers, or inconsistencies** in the data. For example, extreme outliers can skew results for linear models, and unbalanced categorical variables can affect classification outcomes. Detecting these problems early enables you to **clean the data, handle missing values, and apply transformations** to make the data suitable for modeling.\n",
        "## Feature Selection and Engineering  \n",
        "  \n",
        "EDA helps in **selecting relevant features** and creating meaningful new ones. Correlation analysis can reveal highly collinear variables that may negatively impact some models, while visual inspection may suggest transformations (like log or scaling) to handle skewness or heteroscedasticity. This ensures the model is trained on **informative and properly scaled data**, improving predictive accuracy.\n",
        "## Informing Model Choice  \n",
        "  \n",
        "Certain patterns discovered during EDA, such as **non-linear relationships or class imbalance**, can guide your choice of algorithm. For instance, a linear regression may underperform if relationships are highly non-linear, suggesting techniques like decision trees or ensemble methods.\n",
        "## Improving Model Interpretability and Robustness  \n",
        "  \n",
        "By performing EDA, you gain insights into the **underlying structure and peculiarities of the dataset**. This leads to models that are **more interpretable, less prone to overfitting, and better at generalizing** to unseen data.\n",
        "## Summary  \n",
        "  \n",
        "In brief, **EDA is a crucial step before model fitting** because it:  \n",
        "- Provides a deep understanding of the data distributions and patterns    \n",
        "- Detects errors, missing values, outliers, and inconsistencies    \n",
        "- Guides feature selection, engineering, and transformations    \n",
        "- Helps choose the proper modeling algorithms suited for the data    \n",
        "- Increases model interpretability, accuracy, and robustness    \n",
        "  \n",
        "Skipping EDA can result in poor model performance, misleading inferences, and wasted computational resources. Therefore, exploring and cleaning the data should always precede model fitting.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "h2SVrxn3KkIw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is correlation?\n",
        "\n",
        "ans- A correlation coefficient is a numerical measure that quantifies the strength and direction of a linear relationship between two variables. It ranges from -1 to 1, where:\n",
        "\n",
        "+1 indicates a perfect positive correlation,\n",
        "\n",
        "-1 indicates a perfect negative correlation,\n",
        "\n",
        "0 indicates no correlation.\n",
        "\n",
        "Types of Correlation Coefficients\n",
        "\n",
        "Pearson's r\n",
        "\n",
        "The Pearson product-moment correlation coefficient (Pearson's r) measures the linear relationship between two continuous variables. It is calculated as the covariance of the variables divided by the product of their standard deviations. The formula is:\n",
        "\n",
        "-Spearman's rho\n",
        "\n",
        "Spearman's rank correlation coefficient (Spearman's rho) assesses how well the relationship between two variables can be described using a monotonic function. It is used when the data do not meet the assumptions of Pearson's r, such as non-normal distributions or ordinal data.\n",
        "\n",
        "- mportant Considerations\n",
        "\n",
        "Outliers: Correlation coefficients can be distorted by outliers, leading to misleading results\n",
        "1\n",
        ".\n",
        "\n",
        "Causation: Correlation does not imply causation. A high correlation between two variables does not mean that one causes the other\n",
        "1\n",
        ".\n",
        "\n",
        "By understanding and correctly applying correlation coefficients, you can effectively analyze the relationships between variables in your data."
      ],
      "metadata": {
        "id": "zGcaz8cTKoiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def pearson_correlation(x, y):\n",
        "   return np.corrcoef(x, y)[0, 1]"
      ],
      "metadata": {
        "id": "Kxy8wucAR4Nq"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import spearmanr\n",
        "def spearman_correlation(x, y):\n",
        "   return spearmanr(x, y).correlation"
      ],
      "metadata": {
        "id": "-jYsUlx2R5-v"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import kendalltau\n",
        "def kendall_correlation(x, y):\n",
        "   return kendalltau(x, y).correlation"
      ],
      "metadata": {
        "id": "0qsoG6I0R_L4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What does negative correlation mean?\n",
        "\n",
        "\n",
        "ans-\n",
        "**A negative correlation indicates that as one variable increases, the other variable tends to decrease, and vice versa, showing an inverse relationship between the two variables.**\n",
        "  \n",
        "  \n",
        "  \n",
        "### Definition  \n",
        "In statistics, **correlation** measures the strength and direction of a relationship between two variables. The correlation coefficient, often denoted as **r**, ranges from **-1 to +1**:\n",
        "- **r = -1** indicates a perfect negative correlation.  \n",
        "- **r = 0** indicates no linear correlation.  \n",
        "- **r = +1** indicates a perfect positive correlation.  \n",
        "  \n",
        "A **negative correlation** occurs when **r is less than 0**. This means that higher values of one variable are generally associated with lower values of the other variable, and smaller values of one variable are associated with higher values of the other.\n",
        "### Examples  \n",
        "- **Temperature and heating bills**: As outside temperature rises, heating bills usually decrease.    \n",
        "- **Exercise time and body fat percentage**: More time spent exercising often corresponds to lower body fat levels.    \n",
        "- **Speed and travel time on a fixed route**: Traveling faster generally reduces the time needed to complete a trip.  \n",
        "  \n",
        "### Interpretation  \n",
        "- The **closer r is to -1**, the stronger the inverse relationship.    \n",
        "- A weak negative correlation might have **r values near -0.1 or -0.2**, showing a weak but still observable inverse relationship.    \n",
        "- Negative correlation does not imply causation; it only reflects the **statistical association** between two variables.  \n",
        "  \n",
        "Understanding negative correlation helps in predicting how one variable might change as another variable changes, which is useful in data analysis, research, and decision-making.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8cOOj2guKx4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. How can you find correlation between variables in Python?\n",
        "\n",
        "ans-To find the correlation between variables in Python, you can use libraries like NumPy, pandas, or SciPy. Correlation measures the strength and direction of the linear relationship between two variables, with values ranging from -1 to +1.\n",
        "Here are three common methods:\n",
        "\n",
        "1. Using pandas\n",
        "The pandas library provides the .corr() method to calculate correlation coefficients.\n",
        "Pythonimport pandas as pd\n",
        "\n",
        "# Example data\n",
        "data = {'X': [1, 2, 3, 4, 5], 'Y': [2, 4, 6, 8, 10]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate correlation\n",
        "correlation = df['X'].corr(df['Y'])\n",
        "print(\"Correlation (pandas):\", correlation)\n",
        "\n",
        "\n",
        "2. Using NumPy\n",
        "The numpy library provides the np.corrcoef() function to compute the Pearson correlation coefficient.\n",
        "Pythonimport numpy as np\n",
        "\n",
        "# Example data\n",
        "X = [1, 2, 3, 4, 5]\n",
        "Y = [2, 4, 6, 8, 10]\n",
        "\n",
        "# Calculate correlation\n",
        "correlation_matrix = np.corrcoef(X, Y)\n",
        "print(\"Correlation (NumPy):\", correlation_matrix[0, 1])\n",
        "\n",
        "\n",
        "3. Using SciPy\n",
        "The scipy.stats module provides the pearsonr() function for Pearson correlation.\n",
        "Pythonfrom scipy.stats import pearsonr\n",
        "\n",
        "# Example data\n",
        "X = [1, 2, 3, 4, 5]\n",
        "Y = [2, 4, 6, 8, 10]\n",
        "\n",
        "# Calculate correlation\n",
        "correlation, _ = pearsonr(X, Y)\n",
        "print(\"Correlation (SciPy):\", correlation)\n",
        "\n",
        "\n",
        "Each method is suitable depending on your use case. For example:\n",
        "\n",
        "pandas is great for working with DataFrames.\n",
        "NumPy is efficient for numerical arrays.\n",
        "SciPy provides additional statistical details.\n",
        "\n"
      ],
      "metadata": {
        "id": "wanZ_ubkK3Dh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Example data\n",
        "data = {'X': [1, 2, 3, 4, 5], 'Y': [2, 4, 6, 8, 10]}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate correlation\n",
        "correlation = df['X'].corr(df['Y'])\n",
        "print(\"Correlation (pandas):\", correlation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6S9kBNkQIim",
        "outputId": "10b3042e-3402-4b00-a521-bd12e3badf18"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation (pandas): 0.9999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Example data\n",
        "X = [1, 2, 3, 4, 5]\n",
        "Y = [2, 4, 6, 8, 10]\n",
        "\n",
        "# Calculate correlation\n",
        "correlation_matrix = np.corrcoef(X, Y)\n",
        "print(\"Correlation (NumPy):\", correlation_matrix[0, 1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NomPoqBjQN-h",
        "outputId": "4d5389ee-1ee8-4e35-d868-693287eb2320"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation (NumPy): 0.9999999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Example data\n",
        "X = [1, 2, 3, 4, 5]\n",
        "Y = [2, 4, 6, 8, 10]\n",
        "\n",
        "# Calculate correlation\n",
        "correlation, _ = pearsonr(X, Y)\n",
        "print(\"Correlation (SciPy):\", correlation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eyqYxkl3QRjV",
        "outputId": "a6f793cb-4ffb-404f-d7c4-b25e2017be66"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Correlation (SciPy): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is causation? Explain difference between correlation and causation with an example.\n",
        "\n",
        "ans-\n",
        " Causation means that a change in one variable directly results in a change in another variable. For example, if increasing the dosage of a medication leads to a decrease in symptoms, we can say that the medication causes the symptom reduction. In this case, the relationship is clear: A causes B.\n",
        "\n",
        "- Correlation, on the other hand, describes a relationship between two variables where changes in one variable are associated with changes in another variable, but without implying that one causes the other. For instance, there may be a correlation between ice cream sales and the number of people at the beach; as ice cream sales increase, so do beachgoers. However, this does not mean that buying ice cream causes people to go to the beach or vice versa. Instead, a third variable, such as warm weather, influences both.\n",
        "Scribbr\n",
        "+1\n",
        "\n",
        " - Key Differences\n",
        "Nature of Relationship:\n",
        "Causation: Direct cause-and-effect relationship (e.g., smoking causes lung cancer).\n",
        "Correlation: Statistical association without direct causation (e.g., higher ice cream sales correlate with increased beach attendance).\n",
        "Implication:\n",
        "Causation: Implies that if one variable changes, the other will change as a result.\n",
        "Correlation: Does not imply that changes in one variable will cause changes in another; they may simply occur together due to a third factor.\n",
        "2\n",
        "\n",
        "\n",
        "2 Sources\n",
        "Example to Illustrate the Difference\n",
        "Consider the relationship between exercise and weight loss:\n",
        "Causation: Regular exercise leads to weight loss. Here, exercise directly causes a change in weight.\n",
        "Correlation: There may be a correlation between the number of hours spent watching TV and weight gain. However, this does not mean that watching TV causes weight gain; it could be that people who watch more TV tend to exercise less, leading to weight gain due to inactivity.\n",
        "2\n",
        "\n",
        "- Understanding the distinction between correlation and causation is crucial for interpreting data correctly and making informed decisions based on research findings. It helps avoid misleading conclusions that can arise from assuming that correlation implies causation.\n",
        "\n",
        "\n",
        "2 Sources\n",
        "\n",
        "Scribbr\n",
        "Correlation vs. Causation | Difference, Designs & Examples\n",
        "\n",
        "Statistics by Jim\n",
        "Correlation vs Causation: Understanding the Differences - Statistics by Jim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Related searches for What is causation? Explain difference betwee…\n",
        "examples of causation vs correlation\n",
        "difference between correlation causation examples\n",
        "causes and correlation examples\n",
        "correlation differs from causation because\n",
        "concepts of correlation and causality\n",
        "correlation vs causation in research\n",
        "correlation and causation in statistics\n",
        "correlation vs causation psychology\n",
        "PrivacyTerms"
      ],
      "metadata": {
        "id": "mIkGiyKXK9fH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "16.What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
        "\n",
        "ans-    \n",
        "An **optimizer** is an algorithm or method used in **machine learning and deep learning** to adjust the parameters (weights and biases) of a model in order to **minimize or maximize a loss function**. The loss function measures how well the model's predictions match the actual data. Optimizers determine the direction and magnitude of parameter updates during training, guiding the model toward better performance.\n",
        "Formally, if a model has parameters \\\\(\\\\theta\\\\) and a loss function \\\\(L(\\\\theta)\\\\), an optimizer seeks to find:\n",
        "\\\\[\n",
        "\\\\theta^* = \\\\arg\\\\min_{\\\\theta} L(\\\\theta)\n",
        "\\\\]\n",
        "Optimizers are crucial because the efficiency, convergence speed, and stability of training depend heavily on how the algorithm navigates the optimization landscape.\n",
        "---\n",
        "## Types of Optimizers  \n",
        "  \n",
        "Optimizers can be broadly categorized into **Gradient-based** and **non-Gradient-based** methods. In modern deep learning, gradient-based optimizers are most commonly used.\n",
        "### 1. **Stochastic Gradient Descent (SGD)**  \n",
        "SGD updates model parameters using the gradient of the loss with respect to the parameters.\n",
        "**Update Rule:**\n",
        "\\\\[\n",
        "\\\\theta_{t+1} = \\\\theta_t - \\\\eta\n",
        "abla_\\\\theta L(\\\\theta_t)\n",
        "\\\\]\n",
        "where:\n",
        "- \\\\(\\\\eta\\\\) is the learning rate,  \n",
        "- \\\\(  \n",
        "  \n",
        "abla_\\\\theta L(\\\\theta_t)\\\\) is the gradient of the loss function with respect to \\\\(\\\\theta_t\\\\).\n",
        "**Example:**  \n",
        "Training a logistic regression model: after computing the loss on a mini-batch of data, weights are updated in the opposite direction of the gradient to reduce the prediction error.\n",
        "**Pros:** Simple and widely used.  \n",
        "**Cons:** Can be slow and sensitive to the choice of learning rate.\n",
        "---\n",
        "### 2. **Momentum**  \n",
        "Momentum adds a fraction of the previous update to the current update to accelerate optimization and prevent oscillations.\n",
        "**Update Rule:**\n",
        "\\\\[\n",
        "v_{t+1} = \\\\beta v_t + (1 - \\\\beta)\n",
        "abla_\\\\theta L(\\\\theta_t)\n",
        "\\\\]\n",
        "\\\\[\n",
        "\\\\theta_{t+1} = \\\\theta_t - \\\\eta v_{t+1}\n",
        "\\\\]\n",
        "where:\n",
        "- \\\\(v_t\\\\) is the velocity (momentum term),  \n",
        "- \\\\(\\\\beta\\\\) is the momentum coefficient (commonly 0.9).  \n",
        "  \n",
        "**Example:** Training deep networks with SGD often benefits from momentum to escape shallow local minima.\n",
        "---\n",
        "### 3. **AdaGrad (Adaptive Gradient)**  \n",
        "AdaGrad adapts the learning rate for each parameter individually based on the historical gradient.\n",
        "**Update Rule:**\n",
        "\\\\[\n",
        "\\\\theta_{t+1} = \\\\theta_t - \\\\frac{\\\\eta}{\\\\sqrt{G_t + \\\\epsilon}}\n",
        "abla_\\\\theta L(\\\\theta_t)\n",
        "\\\\]\n",
        "where \\\\(G_t\\\\) is the sum of squares of past gradients.\n",
        "**Example:** Natural language processing tasks with sparse features, such as word embeddings, benefit from AdaGrad’s adaptive learning rates.\n",
        "**Pros:** Automatically adjusts learning rates.  \n",
        "**Cons:** Accumulates squared gradients which may slow down learning in the long term.\n",
        "---\n",
        "### 4. **RMSProp**  \n",
        "RMSProp modifies AdaGrad by using a moving average of squared gradients to prevent diminishing learning rates.\n",
        "**Update Rule:**\n",
        "\\\\[\n",
        "E[g^2]_t = \\\\gamma E[g^2]_{t-1} + (1-\\\\gamma) (\n",
        "abla_\\\\theta L(\\\\theta_t))^2\n",
        "\\\\]\n",
        "\\\\[\n",
        "\\\\theta_{t+1} = \\\\theta_t - \\\\frac{\\\\eta}{\\\\sqrt{E[g^2]_t + \\\\epsilon}}\n",
        "abla_\\\\theta L(\\\\theta_t)\n",
        "\\\\]\n",
        "**Example:** Efficient for training recurrent neural networks on sequential data.\n",
        "---\n",
        "### 5. **Adam (Adaptive Moment Estimation)**  \n",
        "Adam combines momentum and RMSProp by maintaining **first-order (mean)** and **second-order (variance)** estimates of gradients.\n",
        "**Update Rules:**\n",
        "\\\\[\n",
        "m_t = \\\\beta_1 m_{t-1} + (1-\\\\beta_1)\n",
        "abla_\\\\theta L(\\\\theta_t)\n",
        "\\\\]\n",
        "\\\\[\n",
        "v_t = \\\\beta_2 v_{t-1} + (1-\\\\beta_2)(\n",
        "abla_\\\\theta L(\\\\theta_t))^2\n",
        "\\\\]\n",
        "\\\\[\n",
        "\\\\hat{m}_t = \\\\frac{m_t}{1-\\\\beta_1^t}, \\\\quad \\\\hat{v}_t = \\\\frac{v_t}{1-\\\\beta_2^t}\n",
        "\\\\]\n",
        "\\\\[\n",
        "\\\\theta_{t+1} = \\\\theta_t - \\\\eta \\\\frac{\\\\hat{m}_t}{\\\\sqrt{\\\\hat{v}_t} + \\\\epsilon}\n",
        "\\\\]\n",
        "**Example:** Most common choice for deep learning architectures such as CNNs and transformers.\n",
        "**Pros:** Fast convergence, handles sparse gradients.  \n",
        "**Cons:** Sensitive to hyperparameters, may sometimes converge to slightly worse minima than SGD.\n",
        "---\n",
        "### 6. **Other Optimizers**  \n",
        "- **Nadam:** Adam + Nesterov momentum.  \n",
        "- **Adadelta:** Improvement of AdaGrad addressing diminishing learning rate problem.  \n",
        "- **LBFGS:** Quasi-Newton method, useful for smaller datasets where exact gradients can be computed efficiently.  \n",
        "  \n",
        "---\n",
        "\n",
        "  \n",
        "  \n",
        "---\n",
        "### References:  \n",
        "- Goodfellow, Bengio, Courville, *Deep Learning*, MIT Press, 2016.  \n",
        "- Kingma & Ba, *Adam: A Method for Stochastic Optimization*, 2015.  \n",
        "  \n",
        "This overview provides a **comprehensive understanding of optimizers**, their mechanisms, and typical applications in machine learning and deep learning.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "V5RZgFjlLFrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17.What is sklearn.linear_model ?\n",
        "\n",
        "ans-  The linear_model module in Scikit-learn provides various linear models for regression and classification tasks. These models are designed to predict target values as a linear combination of input features."
      ],
      "metadata": {
        "id": "6O5ckxnSLMKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import linear_model\n",
        "# Create a Linear Regression model\n",
        "reg = linear_model.LinearRegression()\n",
        "# Fit the model with data\n",
        "reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
        "# Print the coefficients\n",
        "print(reg.coef_) # Output: [0.5, 0.5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0isNgQRmPNZ_",
        "outputId": "4d700d44-d7be-4f87-eee0-d045f599280d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5 0.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg = linear_model.Ridge(alpha=0.5)\n",
        "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
        "print(reg.coef_) # Output: [0.34545455, 0.34545455]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4OJpqXzPQw7",
        "outputId": "16be8f8e-d6da-42d0-a261-31144a8dc81e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.34545455 0.34545455]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg = linear_model.Lasso(alpha=0.1)\n",
        "reg.fit([[0, 0], [1, 1]], [0, 1])\n",
        "print(reg.predict([[1, 1]])) # Output: [0.8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGCn49J4PUPi",
        "outputId": "82445830-64e1-48f0-abd5-0c014c598641"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg = linear_model.LogisticRegression()\n",
        "reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
        "print(reg.predict([[1, 1]])) # Output: [1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feN40j-KPZsH",
        "outputId": "06f64443-5a81-4a1f-e17f-c5f8a4825a75"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18.What does model.fit() do? What arguments must be given?\n",
        "\n",
        "ans-"
      ],
      "metadata": {
        "id": "SMS7c6BXLP05"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.What does model.predict() do? What arguments must be given?\n",
        "\n",
        "ans- model.predict() in Keras is used to make predictions on new or unseen data using a trained machine learning model. It applies the patterns and relationships the model learned during training to the input data and outputs predictions, which could be probabilities, class labels, or other numerical values depending on the task (e.g., classification, regression).\n",
        "Key Features of model.predict()\n",
        "\n",
        "It is typically used after the model has been trained and evaluated.\n",
        "The output format depends on the model's architecture (e.g., softmax for probabilities in classification, raw values for regression).\n",
        "\n",
        "Arguments for model.predict()\n",
        "\n",
        "\n",
        "x (Required):\n",
        "\n",
        "The input data for which predictions are to be made.\n",
        "It can be a NumPy array, TensorFlow tensor, or a dataset object.\n",
        "The shape of x must match the input shape the model was trained on.\n",
        "\n",
        "\n",
        "\n",
        "batch_size (Optional):\n",
        "\n",
        "Specifies the number of samples to process at a time.\n",
        "If not provided, the default batch size is used.\n",
        "\n",
        "\n",
        "\n",
        "verbose (Optional):\n",
        "\n",
        "Controls the verbosity of the output during prediction.\n",
        "0 = silent, 1 = progress bar, 2 = one line per batch.\n",
        "\n",
        "\n",
        "\n",
        "steps (Optional):\n",
        "\n",
        "Total number of steps (batches of samples) to process.\n",
        "Useful when working with generators or datasets.\n",
        "\n",
        "\n",
        "\n",
        "callbacks (Optional):\n",
        "\n",
        "A list of callback functions to apply during prediction.\n",
        "\n",
        "\n",
        "\n",
        "return_std and return_cov (Optional, for probabilistic models):\n",
        "\n",
        "These are used in some advanced models to return uncertainty estimates (standard deviation or covariance).\n",
        "\n",
        "\n",
        "\n",
        "Example Usage\n",
        "Python# Assuming `model` is a trained Keras model\n",
        "predictions = model.predict(x_test)  # x_test is the input data\n",
        "\n",
        "This will return the predictions for the input data x_test.\n"
      ],
      "metadata": {
        "id": "4wyIWx76LUh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.What are continuous and categorical variables?\n",
        "\n",
        "ans- **Continuous variables can take any numerical value within a range, while categorical variables represent discrete groups or categories.**\n",
        "  \n",
        "  \n",
        "  \n",
        "## Continuous Variables  \n",
        "**Continuous variables** are numerical variables that can take an infinite number of values within a given range. These values are measurable and usually obtained through measurement tools or instruments. Continuous variables are suitable for arithmetic operations such as addition, subtraction, and averaging. Examples include:  \n",
        "- Height of a person (e.g., 170.5 cm)    \n",
        "- Temperature in Celsius or Fahrenheit (e.g., 36.6°C)    \n",
        "- Time taken to complete a task (e.g., 12.3 minutes)    \n",
        "- Weight of an object (e.g., 75.8 kg)    \n",
        "  \n",
        "Continuous variables are often visualized using **histograms, line charts, or scatter plots**, which help in understanding their distribution and trends. They are fundamental in regression analysis, correlation studies, and other statistical modeling techniques.  \n",
        "## Categorical Variables  \n",
        "**Categorical variables**, also known as discrete or qualitative variables, represent distinct groups or categories rather than numerical values. These categories cannot be meaningfully measured with arithmetic operations like averaging, but they can be counted. Categorical variables can be further divided into:  \n",
        "- **Nominal variables**: Categories with no natural order (e.g., blood type: A, B, AB, O; gender: male, female, other)    \n",
        "- **Ordinal variables**: Categories with a meaningful order or ranking, but the differences between them are not necessarily uniform (e.g., education level: high school, bachelor's, master's, PhD; customer satisfaction: low, medium, high)    \n",
        "  \n",
        "Categorical variables are typically analyzed using **bar charts, pie charts, or frequency tables**, which display the number of observations in each category. They are commonly used in classification problems, demographic studies, and survey data analysis.  \n",
        "## Summary  \n",
        "In short, **continuous variables measure quantities with a potentially infinite range of values**, whereas **categorical variables classify observations into distinct groups or levels**. Understanding the distinction is essential for selecting appropriate statistical tests, visualization, and data modeling techniques.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6wKbnP5QLYes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is feature scaling? How does it help in Machine Learning?\n",
        "\n",
        "ans- **Feature scaling is a preprocessing step that standardizes the range of independent variables to improve the performance and convergence of machine learning algorithms.**\n",
        "  \n",
        "  \n",
        "  \n",
        "## What is Feature Scaling?  \n",
        "  \n",
        "Feature scaling is the process of **rescaling the values of numerical features** in a dataset so that they fall within a specific range or have certain statistical properties, like zero mean and unit variance. This ensures that every feature contributes equally to the learning process, preventing features with larger magnitudes from dominating those with smaller values.\n",
        "Common methods of feature scaling include:\n",
        "- **Min-Max Scaling (Normalization):** Rescales features to a fixed range, often [0, 1], using the formula    \n",
        "  \n",
        "  \\\\[\n",
        "X_{\\\\text{scaled}} = \\\\frac{X - X_{\\\\min}}{X_{\\\\max}-X_{\\\\min}}\n",
        "\\\\]  \n",
        "This method preserves the relationships between the original data values.\n",
        "- **Standardization (Z-score Scaling):** Transforms features to have a **mean of 0** and **standard deviation of 1** using the formula    \n",
        "  \n",
        "  \\\\[\n",
        "X_{\\\\text{scaled}} = \\\\frac{X - \\\\mu}{\\\\sigma}\n",
        "\\\\]  \n",
        "Standardization is especially useful when features have different units or scales.\n",
        "- **Robust Scaling:** Uses the median and interquartile range, which is less sensitive to outliers:    \n",
        "  \n",
        "  \\\\[\n",
        "X_{\\\\text{scaled}} = \\\\frac{X - \\\\text{median}}{\\\\text{IQR}}\n",
        "\\\\]\n",
        "## Why Feature Scaling Helps in Machine Learning  \n",
        "  \n",
        "1. **Improves Algorithm Convergence:**    \n",
        "  \n",
        "   Gradient-based methods like linear regression, logistic regression, and neural networks converge faster when features are on a similar scale because the gradients are better conditioned.\n",
        "2. **Prevents Bias Toward Large-Valued Features:**    \n",
        "  \n",
        "   Algorithms like **K-Nearest Neighbors (KNN), K-Means clustering, and Support Vector Machines** rely on distance metrics. Without scaling, features with larger ranges disproportionately influence the distance calculations.\n",
        "3. **Enhances Model Accuracy:**    \n",
        "  \n",
        "   Scaling ensures that no single feature dominates during training, improving the model's ability to learn patterns uniformly across all features.\n",
        "4. **Supports Regularization:**    \n",
        "  \n",
        "   Techniques like Lasso and Ridge regression penalize coefficients based on their magnitude. Feature scaling ensures that regularization treats all features fairly.\n",
        "## Conclusion  \n",
        "  \n",
        "In summary, **feature scaling is crucial for ensuring that machine learning models perform optimally**, especially for algorithms sensitive to feature magnitudes or distance measures. Choosing the appropriate scaling method depends on the data distribution, the presence of outliers, and the specific algorithm being used. Proper scaling often leads to faster convergence, improved accuracy, and more reliable predictions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AzpbBeeGLcWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22.How do we perform scaling in Python?\n",
        "\n",
        "ans- Scaling in Python is a crucial step in data preprocessing, especially for machine learning models that are sensitive to the scale of data. Here’s a concise guide on how to perform scaling:\n",
        "\n",
        "1. Standardization\n",
        "Standardization scales data to have a mean of 0 and a standard deviation of 1. It is useful when features have different units or ranges.\n",
        "Pythonfrom sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "2. Min-Max Normalization\n",
        "This scales data to a fixed range, typically [0, 1]. It is useful when you want to preserve the relationships between data points.\n",
        "Pythonfrom sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "\n",
        "\n",
        "3. Robust Scaling\n",
        "RobustScaler uses the median and interquartile range, making it robust to outliers.\n",
        "Pythonfrom sklearn.preprocessing import RobustScaler\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "When to Use Each Method\n",
        "\n",
        "Standardization: When data follows a Gaussian distribution or when the algorithm assumes normally distributed data (e.g., SVM, Logistic Regression).\n",
        "Min-Max Normalization: When you need data in a specific range (e.g., neural networks).\n",
        "Robust Scaling: When the dataset contains significant outliers.\n",
        "\n",
        "These methods are implemented using the scikit-learn library, which is widely used for machine learning tasks.\n"
      ],
      "metadata": {
        "id": "sZOjthh9Lgvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Example data\n",
        "data = [[10, 200], [15, 300], [20, 400]]\n",
        "\n",
        "# Apply StandardScaler\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj5BHVyDNzXB",
        "outputId": "3a95b717-6eac-4a87-be69-f1bdf34dadcc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Example data\n",
        "data = [[10, 200], [15, 300], [20, 400]]\n",
        "\n",
        "# Apply MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puyPAq_tN2cK",
        "outputId": "a569e614-0c8d-4776-be54-50f5f3c31f3f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.  0. ]\n",
            " [0.5 0.5]\n",
            " [1.  1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "# Example data\n",
        "data = [[10, 200], [15, 300], [1000, 400]]  # Contains an outlier\n",
        "\n",
        "# Apply RobustScaler\n",
        "scaler = RobustScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "print(scaled_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVs3ErlTN7v2",
        "outputId": "939a1392-3f48-4e9a-d42d-0aa8d48d7383"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.01010101 -1.        ]\n",
            " [ 0.          0.        ]\n",
            " [ 1.98989899  1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is sklearn.preprocessing?\n",
        "\n",
        "ans- sklearn.preprocessing is a module in the Scikit-learn library in Python that provides a variety of tools and utilities for data preprocessing. Preprocessing is a crucial step in machine learning pipelines, as it transforms raw data into a format that is more suitable for modeling. This module includes methods for scaling, normalizing, encoding, and transforming data to improve the performance of machine learning algorithms.\n",
        "Key Features of sklearn.preprocessing\n",
        "\n",
        "\n",
        "Scaling and Normalization:\n",
        "\n",
        "StandardScaler: Standardizes features by removing the mean and scaling to unit variance.\n",
        "MinMaxScaler: Scales features to a given range, typically [0, 1].\n",
        "MaxAbsScaler: Scales features by their maximum absolute value.\n",
        "Normalizer: Normalizes samples individually to have unit norm.\n",
        "\n",
        "\n",
        "\n",
        "Encoding Categorical Data:\n",
        "\n",
        "OneHotEncoder: Converts categorical features into a one-hot numeric array.\n",
        "LabelEncoder: Encodes target labels with values between 0 and n_classes-1.\n",
        "OrdinalEncoder: Encodes categorical features as integers while preserving the order.\n",
        "\n",
        "\n",
        "\n",
        "Feature Transformation:\n",
        "\n",
        "PolynomialFeatures: Generates polynomial and interaction features.\n",
        "Binarizer: Binarizes data (sets values above a threshold to 1 and others to 0).\n",
        "PowerTransformer: Applies power transformations like Box-Cox or Yeo-Johnson to make data more Gaussian-like.\n",
        "\n",
        "\n",
        "\n",
        "Imputation:\n",
        "\n",
        "SimpleImputer: Handles missing values by replacing them with a specified strategy (mean, median, etc.).\n",
        "KNNImputer: Fills missing values using k-nearest neighbors.\n",
        "\n",
        "\n",
        "\n",
        "Discretization:\n",
        "\n",
        "KBinsDiscretizer: Converts continuous data into discrete bins.\n",
        "\n",
        "\n",
        "\n",
        "Custom Transformations:\n",
        "\n",
        "FunctionTransformer: Allows applying custom transformations to data.\n",
        "\n",
        "\n",
        "\n",
        "Example Usage\n",
        "Pythonfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Example 1: Scaling numerical data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# Example 2: Encoding categorical data\n",
        "encoder = OneHotEncoder()\n",
        "encoded_data = encoder.fit_transform([['cat'], ['dog'], ['cat']]).toarray()\n",
        "\n",
        "print(\"Scaled Data:\\n\", scaled_data)\n",
        "print(\"Encoded Data:\\n\", encoded_data)\n",
        "\n",
        "Why Use sklearn.preprocessing?\n",
        "\n",
        "Ensures data is in the right format for machine learning models.\n",
        "Improves model performance by standardizing or normalizing features.\n",
        "Handles missing or categorical data efficiently.\n",
        "Provides a consistent and easy-to-use API for preprocessing tasks.\n",
        "\n",
        "This module is an essential part of Scikit-learn and is widely used in data preprocessing pipelines.\n"
      ],
      "metadata": {
        "id": "sZjn6-OPLmB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "# Example 1: Scaling numerical data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# Example 2: Encoding categorical data\n",
        "encoder = OneHotEncoder()\n",
        "encoded_data = encoder.fit_transform([['cat'], ['dog'], ['cat']]).toarray()\n",
        "\n",
        "print(\"Scaled Data:\\n\", scaled_data)\n",
        "print(\"Encoded Data:\\n\", encoded_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flhKcdONNQ0F",
        "outputId": "987d5bfd-366d-48b9-ea63-ca3dcf294853"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled Data:\n",
            " [[-1.22474487 -1.22474487]\n",
            " [ 0.          0.        ]\n",
            " [ 1.22474487  1.22474487]]\n",
            "Encoded Data:\n",
            " [[1. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How do we split data for model fitting (training and testing) in Python?\n",
        "\n",
        "ans- To split data into training and testing sets in Python, you can use the train_test_split() function from the sklearn.model_selection module. This function is widely used in machine learning to ensure unbiased model evaluation. Here's how you can do it:\n",
        "Example Code\n",
        "Pythonfrom sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example dataset\n",
        "X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]  # Features\n",
        "y = [0, 1, 0, 1, 0]  # Labels\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Output the results\n",
        "print(\"Training Features:\", X_train)\n",
        "print(\"Testing Features:\", X_test)\n",
        "print(\"Training Labels:\", y_train)\n",
        "print(\"Testing Labels:\", y_test)\n",
        "\n",
        "Key Parameters\n",
        "\n",
        "test_size: Proportion of the dataset to include in the test split (e.g., 0.2 means 20% test data).\n",
        "random_state: Ensures reproducibility by fixing the random seed.\n",
        "shuffle: Whether to shuffle the data before splitting (default is True).\n",
        "\n",
        "Explanation\n",
        "\n",
        "Training Set: Used to train the model.\n",
        "Testing Set: Used to evaluate the model's performance on unseen data.\n",
        "\n",
        "This approach ensures that your model is trained and tested on separate data, reducing the risk of overfitting and providing a more accurate evaluation of its performance.\n"
      ],
      "metadata": {
        "id": "KsRENcwXLrG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example dataset\n",
        "X = [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]  # Features\n",
        "y = [0, 1, 0, 1, 0]  # Labels\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Output the results\n",
        "print(\"Training Features:\", X_train)\n",
        "print(\"Testing Features:\", X_test)\n",
        "print(\"Training Labels:\", y_train)\n",
        "print(\"Testing Labels:\", y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpDT_jHvM5hi",
        "outputId": "d16c5d0b-a919-4b77-d3aa-d33398cb4abe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Features: [[9, 10], [5, 6], [1, 2], [7, 8]]\n",
            "Testing Features: [[3, 4]]\n",
            "Training Labels: [0, 0, 0, 1]\n",
            "Testing Labels: [1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Explain data encoding?\n",
        "\n",
        "ans-Data encoding is the process of converting categorical data inti a numerical format that machine laearning algorithms can understant and process.This is necessary because most machine laering models work with numerical input."
      ],
      "metadata": {
        "id": "22WVhRQpLvZw"
      }
    }
  ]
}